{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SequenceGeneratorFastTextSMOTE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivanggarg1998/Suggestion-Mining/blob/master/SequenceGeneratorFastTextSMOTE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vxpjnqeREvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################################################################################\n",
        "######################### IMPORTING THE LIBRARIES AND DATASET #######################\n",
        "#####################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXnCUswHkR7x",
        "colab_type": "code",
        "outputId": "5d0e26a1-cefa-41c5-aca6-22257d7374a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aGRdIsTkoDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Suggestion-Mining2/master')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly7fty8sRabJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# FOR PLOTTING GRAPHS\n",
        "import matplotlib.pyplot as plt\n",
        "pd.set_option('display.max_colwidth',300)\n",
        "\n",
        "# FOR REMOVING SPECIAL CHARACTERS, LINKS, AND EXPANDING WORDS\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# FOR STEMMING AND REMOVING STOP WORDS\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer   \n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "\n",
        "# FOR BUILDING THE EMBEDDING MATRIX AND GENERATING THE SEQUENCES\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# FOR THE FAST TEST WORD VECTOR WEIGHTS\n",
        "from tqdm import tqdm\n",
        "import codecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcT_qFibRkHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORTING THE DATASET\n",
        "train_data=pd.read_csv(\"./Data/TrainingData.csv\")\n",
        "test_data=pd.read_csv(\"./Data/SubtaskA_Trial_Test_Labeled.csv\")\n",
        "valid_data=pd.read_csv(\"./Data/SubtaskA_EvaluationData_labeled.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_2TChEuRt53",
        "colab_type": "code",
        "outputId": "408e2bba-3903-4369-cfcb-337a48f307d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "# PEEKING INTO THE TRAIN DATA\n",
        "print(train_data.shape)\n",
        "train_data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8500, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>663_3</td>\n",
              "      <td>\"Please enable removing language code from the Dev Center \"language history\" For example if you ever selected \"ru\" and \"ru-ru\" laguages and you published this xap to the Store then it causes Tile localization to show the en-us(default) tile localization which is bad.\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>663_4</td>\n",
              "      <td>\"Note: in your .csproj file, there is a SupportedCultures entry like this: &lt;SupportedCultures&gt;de-DE;ru;ru-RU &lt;/SupportedCultures&gt; When I removed the \"ru\" language code and published my new xap version, the old xap version still remains in the Store with \"Replaced and unpublished\".\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>664_1</td>\n",
              "      <td>\"Wich means the new version not fully replaced the old version and this causes me very serious problems: 1.\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>664_2</td>\n",
              "      <td>\"Some of my users will still receive the old xap version of my app.\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>664_3</td>\n",
              "      <td>\"The store randomly gives the old xap or the new xap version of my app.\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ... label\n",
              "0  663_3  ...     1\n",
              "1  663_4  ...     0\n",
              "2  664_1  ...     0\n",
              "3  664_2  ...     0\n",
              "4  664_3  ...     0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1pVvsvWRyUW",
        "colab_type": "code",
        "outputId": "a2ae21fd-9b40-41ef-ebac-f9c8a246b6c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "# PEEKING INTO THE TEST DATA\n",
        "print(test_data.shape)\n",
        "test_data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(592, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1310_1</td>\n",
              "      <td>I'm not asking Microsoft to Gives permission like Android so any app can take my data, but don't keep it restricted like iPhone.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1312_1</td>\n",
              "      <td>somewhere between Android and iPhone.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1313_1</td>\n",
              "      <td>And in the Windows Store you can flag the App [Requires Trust] for example.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1313_2</td>\n",
              "      <td>Many thanks Sameh Hi, As we know, there is a lot of limitations is WP8 OS due the high security in the OS itself which is very good, but some time we need to allow some apps to do extra works, apps which we trust i.e: hotmail app, facebook app, skype app ....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1313_3</td>\n",
              "      <td>The idea is that we can develop a regular app and we request our permissions in the manifest, OR the app can ASK FOR TRUST_�_ more</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ... label\n",
              "0  1310_1  ...     1\n",
              "1  1312_1  ...     0\n",
              "2  1313_1  ...     0\n",
              "3  1313_2  ...     0\n",
              "4  1313_3  ...     1\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhYQJmXTRzcP",
        "colab_type": "code",
        "outputId": "b11d63c9-6e10-48cd-a3c9-691fe746abaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "# PEEKING INTO THE VALIDATION DATA\n",
        "print(valid_data.shape)\n",
        "valid_data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(833, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9566</td>\n",
              "      <td>This would enable live traffic aware apps.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9569</td>\n",
              "      <td>Please try other formatting like bold italics shadow to distinguish titles/subtitles from content.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9576</td>\n",
              "      <td>Since computers were invented to save time I suggest we be allowed to upload them all in one zip file - using numbering for the file names and the portal could place them in the right order.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9577</td>\n",
              "      <td>Allow rearranging if the user wants to change them!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9579</td>\n",
              "      <td>Add SIMD instructions for better use of ARM NEON instructions for math and games.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id  ... label\n",
              "0  9566  ...     0\n",
              "1  9569  ...     1\n",
              "2  9576  ...     1\n",
              "3  9577  ...     1\n",
              "4  9579  ...     1\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S27MHMdNR0tY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################################################################################\n",
        "##########################        CLEANING THE DATA        ##########################\n",
        "#####################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgferb94R2EM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONTRACTION_MAP = {\"ain't\": \"is not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he he will have\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"i'd\": \"i would\",\"i'd've\": \"i would have\",\"i'll\": \"i will\",\"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so as\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there would\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\"you're\": \"you are\",\"you've\": \"you have\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPMc1HFBR3Vn",
        "colab_type": "code",
        "outputId": "72798057-3c7b-4089-a20d-626e2e5a4f64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "# nltk.download('stopwords')\n",
        "#### CACHING THE STOP WORDS HELPS IN FASTENING THE REMOVAL OF THE STOP WORDS\n",
        "# cachedStopWords = stopwords.words(\"english\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "corpus_words = set(nltk.corpus.words.words())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxYnnfrjR4qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def cleanData(data):\n",
        "\n",
        "    ## REMOVING ASCENTED CHARACTERS LIKE é\n",
        "    def removeAscentedCharacters(text):\n",
        "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        return text\n",
        "    \n",
        "    \n",
        "    ## EXPANDING THE SHORT WORDS:\n",
        "    def expandContractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "        contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                          flags=re.IGNORECASE|re.DOTALL)\n",
        "        def expand_match(contraction):\n",
        "            match = contraction.group(0)\n",
        "            first_char = match[0]\n",
        "            expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                    if contraction_mapping.get(match)\\\n",
        "                                    else contraction_mapping.get(match.lower())                       \n",
        "            expanded_contraction = first_char+expanded_contraction[1:]\n",
        "            return expanded_contraction\n",
        "            \n",
        "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "        return expanded_text\n",
        "    \n",
        "    ## REMOVING FRONT AND BACK INVERTED COMMAS\n",
        "    def removeIC(text):\n",
        "        if len(text)>=2:\n",
        "          if text[0]=='\"':\n",
        "            text = text[1:]\n",
        "          if text[-1]=='\"':\n",
        "            text = text[:-1]\n",
        "        return text\n",
        "    \n",
        "    ## REMOVING TAGS\n",
        "    def remove_tags(text):\n",
        "        soup = BeautifulSoup(text)\n",
        "        return soup.get_text()\n",
        "\n",
        "    def deEmojify(inputString):\n",
        "        return inputString.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "    def removeSpaces(text):\n",
        "        text= re.sub(' +', ' ', text)\n",
        "        if text[0]==' ':\n",
        "          text=text[1:]\n",
        "        return text\n",
        "\n",
        "    def get_simple_pos(tag):\n",
        "        if tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return wordnet.NOUN  \n",
        "\n",
        "    # OUR STEMMING FUNCTION\n",
        "    def stem(words):\n",
        "      output_words=[]\n",
        "      if len(words)!=0:\n",
        "        words[0] = words[0].lower()\n",
        "      for w in words:\n",
        "              pos=pos_tag([w])\n",
        "              simple_pos = get_simple_pos(pos[0][1])\n",
        "              clean_word=lemmatizer.lemmatize(w,simple_pos)\n",
        "              output_words.append(clean_word.lower())\n",
        "      return output_words\n",
        "\n",
        "    def stemmizeSentence(sentence):\n",
        "      output_words = stem(sentence)\n",
        "      output_wordsf = []\n",
        "      for i in output_words:\n",
        "        if i in corpus_words:\n",
        "          output_wordsf.append(i)\n",
        "      return output_wordsf\n",
        "\n",
        "    print('REMOVING ASCENTED CHARACTERS...')\n",
        "    cleaned = data.apply(lambda x: removeAscentedCharacters(x))\n",
        "    print('NORMALIZING THE SENTENCE CASE...')\n",
        "    cleaned = cleaned.apply(lambda x: x.lower())\n",
        "    print('EXPANDING CONTRACTIONS...')\n",
        "    cleaned = cleaned.apply(lambda x: expandContractions(x))\n",
        "    print('REMOVING IC...')\n",
        "    cleaned=  cleaned.apply(lambda x: removeIC(x))\n",
        "    print('REMOVING TAGS...')\n",
        "    cleaned = cleaned.apply(lambda x: remove_tags(x))\n",
        "    print('REMOVING LINKS...')\n",
        "    cleaned = cleaned.str.replace(\"(https?:\\/\\/)(\\s)(www\\.)?(\\s)((\\w|\\s)+\\.)([\\w\\-\\s]+\\/)([\\w\\-]+)((\\?)?[\\w\\s]=\\s[\\w\\%&])\",\" \")\n",
        "    print('REMOVING SPECIAL CHARACTERS...')\n",
        "    cleaned = cleaned.str.replace(\"\\\".?\\\"|\\(.?\\)|<.*>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});|[^a-zA-Z#]\",\" \")\n",
        "    print('REMOVING EMOJIS...')\n",
        "    cleaned = cleaned.apply(lambda x: deEmojify(x))\n",
        "    print('REMOVING UNNECCESSARY SPACES...')\n",
        "    cleaned = cleaned.apply(lambda x: removeSpaces(x))\n",
        "    print('REMOVING THE CONSECUTIVELY REPEATED WORDS...')\n",
        "    cleaned = cleaned.apply(lambda x: re.sub(r'\\b(.+)\\s+\\1\\b', r'\\1', x))\n",
        "    print('REMOVING STOP WORDS...')\n",
        "    tokenized_sentence = cleaned.apply(lambda x: x.split())\n",
        "    tokenized_sentence = tokenized_sentence.apply(lambda sentence: [word for word in sentence if len(word)>2 ])\n",
        "    # tokenized_sentence = tokenized_sentence.apply( lambda sentence: [word for word in sentence if word not in cachedStopWords] )\n",
        "    print('TOKENIZING AND STEMMING...')\n",
        "    tokenized_sentence = tokenized_sentence.apply(lambda sentence: stemmizeSentence(sentence))\n",
        "    print('FINALIZING THE DATA')\n",
        "    detokenized= tokenized_sentence.apply(lambda x: ' '.join(x))\n",
        "\n",
        "    return detokenized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCrQjj0VR6HC",
        "colab_type": "code",
        "outputId": "99663869-8ab2-40a9-df8b-97419a086dbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        }
      },
      "source": [
        "x_train = cleanData(train_data['sentence'])\n",
        "print(x_train.head())\n",
        "print(x_train.shape)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REMOVING ASCENTED CHARACTERS...\n",
            "NORMALIZING THE SENTENCE CASE...\n",
            "EXPANDING CONTRACTIONS...\n",
            "REMOVING IC...\n",
            "REMOVING TAGS...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://windowsphone.uservoice.com/forums/101801-feature-suggestions/suggestions/6080912-we-want-open-the-app-by-tap-the-quick-status-icon\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://forums.wpcentral.com/windows-phone-apps/235446-wi-fi-analyzer-possible-wp8.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.pitorque.de/mistergoodcat/post/somethings-missing-from-the-webbrowser-control.aspx\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://wpdev.uservoice.com/forums/110705-app-platform/suggestions/1908267-extend-the-api-to-query-phone-volume-and-vibration\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://connect.microsoft.com/visualstudio/feedback/details/613932/windows-phone-7-visual-studio-generates-uncompilable-code-when-adding-an-image-to-a-resource-file\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://wpdev.uservoice.com/forums/110705-app-platform/suggestions/1908309-enable-microsoft-xna-framework-media-visualization\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://yves.vg/testcases/ie_mobi/keyup_event.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://mrreaderblog.curioustimes.de/post/52060909928/supported-google-reader-alternatives-part-two\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://getpocket.com/developer/docs/authentication\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://msdn.microsoft.com/en-us/library/windowsphone/develop/jj681688(v=vs.105).aspx\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "REMOVING LINKS...\n",
            "REMOVING SPECIAL CHARACTERS...\n",
            "REMOVING EMOJIS...\n",
            "REMOVING UNNECCESSARY SPACES...\n",
            "REMOVING THE CONSECUTIVELY REPEATED WORDS...\n",
            "REMOVING STOP WORDS...\n",
            "TOKENIZING AND STEMMING...\n",
            "FINALIZING THE DATA\n",
            "0    please enable remove language code from the dev center language history for example you ever select and and you publish this the store then cause tile localization show the default tile localization which bad\n",
            "1                                                     note your file there entry like this when remove the language code and publish new version the old version still remains the store with replace and unpublished\n",
            "2                                                                                                                          mean the new version not fully replace the old version and this cause very serious problem\n",
            "3                                                                                                                                                                        some user will still receive the old version\n",
            "4                                                                                                                                                                     the store randomly give the old the new version\n",
            "Name: sentence, dtype: object\n",
            "(8500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuNU45A1R8ol",
        "colab_type": "code",
        "outputId": "58a44c1f-1a6a-4036-fc23-5c1576d0811f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "x_valid = cleanData(valid_data['sentence'])\n",
        "print(x_valid.head())\n",
        "print(x_valid.shape)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REMOVING ASCENTED CHARACTERS...\n",
            "NORMALIZING THE SENTENCE CASE...\n",
            "EXPANDING CONTRACTIONS...\n",
            "REMOVING IC...\n",
            "REMOVING TAGS...\n",
            "REMOVING LINKS...\n",
            "REMOVING SPECIAL CHARACTERS...\n",
            "REMOVING EMOJIS...\n",
            "REMOVING UNNECCESSARY SPACES...\n",
            "REMOVING THE CONSECUTIVELY REPEATED WORDS...\n",
            "REMOVING STOP WORDS...\n",
            "TOKENIZING AND STEMMING...\n",
            "FINALIZING THE DATA\n",
            "0                                                                                                                   this would enable live traffic aware\n",
            "1                                                                       please try other format like bold shadow distinguish title subtitle from content\n",
            "2    since computer be invent save time suggest allow them all one zip file use number for the file name and the portal could place them the right order\n",
            "3                                                                                                              allow rearrange the user want change them\n",
            "4                                                                                    add instruction for well use arm neon instruction for math and game\n",
            "Name: sentence, dtype: object\n",
            "(833,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNeLmlYDR-4J",
        "colab_type": "code",
        "outputId": "72ad22a1-3278-4062-d150-53664d8b88ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "x_test = cleanData(test_data['sentence'])\n",
        "print(x_test.head())\n",
        "print(x_test.shape)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REMOVING ASCENTED CHARACTERS...\n",
            "NORMALIZING THE SENTENCE CASE...\n",
            "EXPANDING CONTRACTIONS...\n",
            "ERROR! Session/line number was not unique in database. History logging moved to new session 59\n",
            "REMOVING IC...\n",
            "REMOVING TAGS...\n",
            "REMOVING LINKS...\n",
            "REMOVING SPECIAL CHARACTERS...\n",
            "REMOVING EMOJIS...\n",
            "REMOVING UNNECCESSARY SPACES...\n",
            "REMOVING THE CONSECUTIVELY REPEATED WORDS...\n",
            "REMOVING STOP WORDS...\n",
            "TOKENIZING AND STEMMING...\n",
            "FINALIZING THE DATA\n",
            "0                                                              not ask give permission like android any can take data but not keep restrict like\n",
            "1                                                                                                                  somewhere between android and\n",
            "2                                                                                and the window store you can flag the require trust for example\n",
            "3    many thanks know there lot limitation due the high security the itself which very good but some time need allow some extra work which trust\n",
            "4                                           the idea that can develop regular and request our permission the manifest the can ask for trust more\n",
            "Name: sentence, dtype: object\n",
            "(592,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98Cz9NIkSAUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=train_data['label']\n",
        "y_valid=valid_data['label']\n",
        "y_test=test_data['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFbKAkGlSBiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################################################################################\n",
        "########################## BUILDING THE EMBEDDING MATRIX   ##########################\n",
        "#####################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ow39BcMSCwc",
        "colab_type": "code",
        "outputId": "2407b59c-8cfb-448a-835d-13cc788cb039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# BUILDING VOCABULARY FROM THE SENTENCES\n",
        "# THIS WILL HELP IN GETTING THE INPUT SEQUENCES FOR THE \n",
        "mxlen=0\n",
        "tokenized=x_train.apply(lambda x: x.split())\n",
        "for tokens in tokenized:\n",
        "  mxlen=max(mxlen,len(tokens))\n",
        "print('MAX LEN', mxlen)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAX LEN 127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDKrwthMSETX",
        "colab_type": "code",
        "outputId": "b3a1c8a4-d404-4aac-d82f-c0253709c19a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "all_words= [ word for tokens in tokenized for word in tokens]\n",
        "vocab = sorted(list(set(all_words)))\n",
        "vocab_train_len=len(vocab)\n",
        "print('VOCAB SIZE',len(vocab))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VOCAB SIZE 3904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XfegBc5SFgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MAX LEN OF AN INPUT SEQUENCE\n",
        "MXSEQLEN=126\n",
        "# GOOGLE NEWS WORD VECTOR ENCODING SIZE\n",
        "MAX_NB_WORDS = 100000\n",
        "EMBEDDING_DIM = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOMCY2hiSGod",
        "colab_type": "code",
        "outputId": "4805133e-1c52-451f-dc2f-ff27dbe595f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# BUILDING TOKENIZER FROM THE TRAINING DATA\n",
        "tokenizer = Tokenizer(num_words=vocab_train_len, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(x_train.tolist())\n",
        "print('Found %s unique tokens.' % len(tokenizer.word_index))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3904 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPlio5R55BL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################################################################################\n",
        "########################## USING SMOTE TO BALANCE THE DATASET   ##########################\n",
        "#####################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiMVZn0w5H5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imblearn.pipeline import make_pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smt = SMOTE(k_neighbors=200,random_state=2,sampling_strategy=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqpos2MRSIAn",
        "colab_type": "code",
        "outputId": "b73dcf27-4123-4e56-9c06-ca13d3b657e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "# form the sequences that will be the input to the network\n",
        "# padd or remove values to make sequences of equal length\n",
        "train_word_index= tokenizer.word_index\n",
        "train_sequence = tokenizer.texts_to_sequences(x_train.tolist())\n",
        "train_sequence = sequence.pad_sequences(train_sequence, maxlen=MXSEQLEN)\n",
        "y_train=train_data['label']\n",
        "train_sequence,y_train = smt.fit_resample(train_sequence,y_train)\n",
        "print(train_sequence)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0 ...  839   34  330]\n",
            " [   0    0    0 ...  630    2 1735]\n",
            " [   0    0    0 ...   80  806   87]\n",
            " ...\n",
            " [   0    0    0 ...   35  139   85]\n",
            " [   0    0    0 ...   65 1355 1261]\n",
            " [   0    0    0 ...  280  866  188]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q88TKoUHSKH5",
        "colab_type": "code",
        "outputId": "b8602155-28f0-4b0d-881e-871741f8d3db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# TEST SEQUENCE BUILT FROM THE SAME TRAINING VOCABULARY\n",
        "test_sequence = tokenizer.texts_to_sequences(x_test.tolist())\n",
        "test_sequence = sequence.pad_sequences(test_sequence, maxlen=MXSEQLEN)\n",
        "print(test_sequence)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0 ...  287 1130   17]\n",
            " [   0    0    0 ...  244  116    2]\n",
            " [   0    0    0 ...  713    3   79]\n",
            " ...\n",
            " [   0    0    0 ...  288    3  248]\n",
            " [   0    0    0 ...    1  268  200]\n",
            " [   0    0    0 ...  160    1  344]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7f_q5fnSLpE",
        "colab_type": "code",
        "outputId": "d7b762e3-3d6e-4b79-9273-f61ef6a1d32b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# TEST SEQUENCE BUILT FROM THE SAME TRAINING VOCABULARY\n",
        "valid_sequence = tokenizer.texts_to_sequences(x_valid.tolist())\n",
        "valid_sequence = sequence.pad_sequences(valid_sequence, maxlen=MXSEQLEN)\n",
        "print(valid_sequence)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0 ...  283 1510 1405]\n",
            " [   0    0    0 ... 1404   20  129]\n",
            " [   0    0    0 ...    1  104  388]\n",
            " ...\n",
            " [   0    0    0 ...   12   80  828]\n",
            " [   0    0    0 ...    6   60   85]\n",
            " [   0    0    0 ...    8  194  842]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXTZKYeFSNNI",
        "colab_type": "code",
        "outputId": "4abbb51b-f752-4f94-fcc0-c08a80576fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "# DOWNLOADING THE PRETRAINED FAST TEXT WORD TO VECTOR REPRESENTATIONS\n",
        "!wget -P /content/ -c https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-07 14:17:13--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 2606:4700:10::6816:4a8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: ‘/content/wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M  20.9MB/s    in 29s     \n",
            "\n",
            "2020-04-07 14:17:43 (22.2 MB/s) - ‘/content/wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
            "\n",
            "UnZip 6.00 of 20 April 2009, by Debian. Original by Info-ZIP.\n",
            "\n",
            "Usage: unzip [-Z] [-opts[modifiers]] file[.zip] [list] [-x xlist] [-d exdir]\n",
            "  Default action is to extract files in list, except those in xlist, to exdir;\n",
            "  file[.zip] may be a wildcard.  -Z => ZipInfo mode (\"unzip -Z\" for usage).\n",
            "\n",
            "  -p  extract files to pipe, no messages     -l  list files (short format)\n",
            "  -f  freshen existing files, create none    -t  test compressed archive data\n",
            "  -u  update files, create if necessary      -z  display archive comment only\n",
            "  -v  list verbosely/show version info       -T  timestamp archive to latest\n",
            "  -x  exclude files that follow (in xlist)   -d  extract files into exdir\n",
            "modifiers:\n",
            "  -n  never overwrite existing files         -q  quiet mode (-qq => quieter)\n",
            "  -o  overwrite files WITHOUT prompting      -a  auto-convert any text files\n",
            "  -j  junk paths (do not make directories)   -aa treat ALL files as text\n",
            "  -U  use escapes for all non-ASCII Unicode  -UU ignore any Unicode fields\n",
            "  -C  match filenames case-insensitively     -L  make (some) names lowercase\n",
            "  -X  restore UID/GID info                   -V  retain VMS version numbers\n",
            "  -K  keep setuid/setgid/tacky permissions   -M  pipe through \"more\" pager\n",
            "  -O CHARSET  specify a character encoding for DOS, Windows and OS/2 archives\n",
            "  -I CHARSET  specify a character encoding for UNIX and other archives\n",
            "\n",
            "See \"unzip -hh\" or unzip.txt for more help.  Examples:\n",
            "  unzip data1 -x joe   => extract all files except joe from zipfile data1.zip\n",
            "  unzip -p foo | more  => send contents of foo.zip via pipe into program more\n",
            "  unzip -fo foo ReadMe => quietly replace existing ReadMe if archive file newer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJAPiFAfAWxb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f1286570-961c-4edb-b1c2-bc035032bdef"
      },
      "source": [
        "!unzip  /content/wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUeoE7IxSUR1",
        "colab_type": "code",
        "outputId": "6f06884e-135e-4974-c339-1d2397941790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print('loading word embeddings...')\n",
        "embeddings_index = {}\n",
        "f = codecs.open('./wiki-news-300d-1M.vec', encoding='utf-8')\n",
        "for line in tqdm(f):\n",
        "    values = line.rstrip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('found %s word vectors' % len(embeddings_index))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loading word embeddings...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "999995it [01:46, 9375.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "found 999995 word vectors\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WukOiHCwSfjL",
        "colab_type": "code",
        "outputId": "a3587019-627d-4e21-a15c-bc9b280eb9d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#embedding matrix\n",
        "print('preparing embedding matrix...')\n",
        "words_not_found = []\n",
        "nb_words = min(MAX_NB_WORDS, len(train_word_index)+1)\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "for word, i in train_word_index.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        words_not_found.append(word)\n",
        "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preparing embedding matrix...\n",
            "number of null word embeddings: 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRRv2cPvSh9n",
        "colab_type": "code",
        "outputId": "cbc53a6c-9612-4e99-8071-a5207e1f2c41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(\"sample words not found: \", np.random.choice(words_not_found, 10))\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample words not found:  ['unabstract' 'reeder' 'unabstract' 'reeder' 'feedy' 'feedy' 'wup'\n",
            " 'actable' 'actable' 'wup']\n",
            "(3905, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9XEj9hRSnW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "variables = {\n",
        "    'train_sequence' : train_sequence,\n",
        "    'test_sequence' : test_sequence,\n",
        "    'valid_sequence' :valid_sequence,\n",
        "    'y_train':pd.Series(y_train),\n",
        "    'y_test':y_test,\n",
        "    'y_valid':y_valid,\n",
        "    'train_embedding_weights':embedding_matrix,\n",
        "    'EMBEDDING_DIM':EMBEDDING_DIM,\n",
        "    'MXSEQLEN':MXSEQLEN,\n",
        "    'train_word_index':train_word_index\n",
        "}\n",
        "\n",
        "pickle.dump(variables,open('./Data/variablesFastTextSMOTE','wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL4ysPJbl_nR",
        "colab_type": "code",
        "outputId": "6ff08dc1-95cb-4f42-8a36-e72e15e2abe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(y_train)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12830"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    }
  ]
}